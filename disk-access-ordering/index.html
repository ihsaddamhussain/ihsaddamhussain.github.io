<!DOCTYPE html>
<html lang="en">
  <head>
  <title>Ordering Requests to Accelerate Disk I/O | Piotr Kołaczkowski</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="In the earlier post I showed how accessing data on an SSD in parallel can greatly improve read performance. However, that technique is not very effective for...">
  <meta name="author" content="Piotr Kołaczkowski">
  <meta name="generator" content="Jekyll v4.1.1">
  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-PBJQD2M');</script>
<!-- End Google Tag Manager -->

  <link rel="canonical" href="/disk-access-ordering/">
  
  <link rel="stylesheet" href="/assets/css/index.css">
  
  <link rel="stylesheet" href="/assets/css/classes.css">
  <link rel="stylesheet" href="/assets/css/sidebar.css" media="screen and (min-width: 70em)">
  <link rel="alternate" href="/feed.xml" type="application/atom+xml" title="Piotr Kołaczkowski">
  
  
  <script defer src="/assets/node_modules/chart.js/dist/Chart.min.js"></script>
</head>
<body>
  <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBJQD2M"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
  <header class="icons">
    
      <a href="/" class="title">Piotr Kołaczkowski</a>
    
    
      
  <nav>
  <a aria-label="Home" href="/" ><svg aria-hidden="true" class="hidden"><use xlink:href="/assets/fontawesome/icons.svg#home"></use></svg><span aria-hidden="true" >Home</span></a>
  <a aria-label="About" href="/about/" ><svg aria-hidden="true" class="hidden"><use xlink:href="/assets/fontawesome/icons.svg#address-card"></use></svg><span aria-hidden="true" >About</span></a>
  
  </nav>


      
  <nav>
  <a aria-label="Mail" href="mailto:pkolaczk@gmail.com" ><svg aria-hidden="true" ><use xlink:href="/assets/fontawesome/icons.svg#envelope"></use></svg><span aria-hidden="true" class="hidden">Mail</span></a>
  <a aria-label="Github" href="https://github.com/pkolaczk" ><svg aria-hidden="true" ><use xlink:href="/assets/fontawesome/icons.svg#github"></use></svg><span aria-hidden="true" class="hidden">Github</span></a>
  <a aria-label="LinkedIn" href="https://www.linkedin.com/in/pkolaczk" ><svg aria-hidden="true" ><use xlink:href="/assets/fontawesome/icons.svg#linkedin"></use></svg><span aria-hidden="true" class="hidden">LinkedIn</span></a>
  <a aria-label="RSS" href="/feed.xml" ><svg aria-hidden="true" ><use xlink:href="/assets/fontawesome/icons.svg#rss"></use></svg><span aria-hidden="true" class="hidden">RSS</span></a>
  
  </nav>


    
    
      <div class="hidden description">Blog on programming, optimization and performance analysis</div>
    

  </header>

  <article>
  <header>
  
  <h1><a href="/disk-access-ordering/">Ordering Requests to Accelerate Disk I/O</a></h1><time datetime="2021-04-04T00:00:00+02:00">April 04, 2021</time>
</header>

  <p>In <a href="/disk-parallelism/">the earlier post</a> I showed how accessing data on
an SSD in parallel can greatly improve read performance. However, that technique
is not very effective for data stored on spinning drives. In some cases parallel access
can even deteriorate performance significantly. Fortunately, there exists a class of optimizations
that can strongly help with HDDs: request ordering. By requesting data in proper order,
the disk seek latency can be reduced by an order of magnitude. Since I introduced that 
optimization in <a href="https://github.com/pkolaczk/fclones">fclones 0.9</a>, <code class="language-plaintext highlighter-rouge">fclones</code> became the 
fastest duplicate file finder I know of.</p>

<!--more-->

<h1 id="hdd-performance-characteristics">HDD Performance Characteristics</h1>

<p>Spinning drives, contrary to SSDs, have significant access latency, limiting the effective
number of IO operations per second they can serve. The disk access latency is mostly comprised of:</p>
<ul>
  <li>seek latency – the time needed to position the head on the desired track,</li>
  <li>rotational latency – the time needed to wait for the disk to rotate so that the right sector is under the head,</li>
</ul>

<p>The seek latency is higher the more distance the head has to move to get to the right track. 
The typical average seek latency advertised by manufacturers are about 5-12 ms. 
The average rotational latency is equal to the time needed for the disk plates to do half of the turn.
In case of a 7200 RPM drive, this equals to 60/7200/2 = 4.2 ms. Overall the total average 
latency can be about 10 ms, and worst-case latency more than 20 ms.</p>

<p>Now if we want to process a bunch of tiny files placed randomly on the disk, and we access them in random
order, we should not expect to process vastly more than about 100 per second 
(you might get lucky though, that some of your files would be located close to each other which may improve it a bit).</p>

<p>This back-of-the envelope calculation holds pretty well in the real world. 
Here is an example <code class="language-plaintext highlighter-rouge">iostat</code> output while searching for duplicates
on a 7200 RPM HDD with an old version of <code class="language-plaintext highlighter-rouge">fclones</code> (0.8):</p>

<pre>
Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda             127,40       655,20         0,00       3276          0
sdb               0,60         0,00        67,20          0        336

Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda             135,00       659,20         0,00       3296          0
sdb              26,00         0,00       174,40          0        872

Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda             132,60       669,60         0,00       3348          0
sdb               0,40         0,00         8,00          0         40

Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda             127,40       683,20         0,00       3416          0
sdb               0,40         2,40        28,00         12        140
</pre>

<p>We can see the number of transactions per second (<code class="language-plaintext highlighter-rouge">tps</code>) is slightly more than 100. That’s a 
ridiculously low number considering SSDs can handle tens or even hundreds thousands random accesses 
per second. The data rate is also essentially “killed” by latency. This drive can read at more than
100 MB/s continuously, but here we get a rate in the range of hundreds of kilobytes.</p>

<p>Unfortunately, even in 2021 you may still have plenty of gigabytes in small files lying 
around on older HDDs. Does it mean you’re doomed to hours of waiting if you ever want to 
process all of them (e.g. search, backup or deduplicate)?</p>

<h1 id="basic-request-ordering">Basic Request Ordering</h1>

<p>In <a class="citation" href="#Lunde2009">[1]</a>
the authors presented some nice techniques of improving performance by sorting I/O requests before
sending them to the operating system for execution. I implemented them in <code class="language-plaintext highlighter-rouge">fclones</code> and the results are
truly amazing!</p>

<p>Up to version 0.8, <code class="language-plaintext highlighter-rouge">fclones</code> processed files in the order dictated by their <em>size</em>, because that was the order
naturally obtained from the first phase of grouping. As you may expect, it turns out, 
file size isn’t correlated with the physical location of a file at all. Hence, the performance on HDD was actually
worse than as if the files were processed in the order obtained from scanning the directory tree. 
At least, when processing files in the order returned by the directory listing, there are high 
chances they were saved at the similar time (e.g. as a result of a directory copy operation) and are actually
placed very close to each other. And indeed,
some alternative programs like <code class="language-plaintext highlighter-rouge">fdupes</code> or <code class="language-plaintext highlighter-rouge">rdfind</code> outperformed <code class="language-plaintext highlighter-rouge">fclones</code> on HDD, despite not really doing anything special
to speed up disk access.</p>

<p>One of the first ideas I tried from the paper was to reorder the files by their inode identifiers. 
This was quite easy, because the inode identifiers were available already in the file metadata structures in order to properly detect
hard-links. Honestly, I wasn’t expecting much improvement from this technique, as theoretically the inode number of a file
has nothing to do with the physical data location. 
In practice though, there seems to be a lot of correlation. This technique alone worked like a charm, despite some minor added cost
of sorting the entries!</p>

<script type="text/javascript" src="/assets/graphs/graphs.js"></script>

<div class="figure">
    <div style="height:7em">
        <canvas id="inodeOrdering"></canvas>
    </div>
    <script>
    makeBarChartDeferred("inodeOrdering", "time [s]", "ordering",
        ["by size", "by inode"],
        {"time": [217, 28.43]});
    </script>
    <span class="caption"> Fig.1: Time to find duplicates among 50k+ files stored on a 7200 RPM drive</span>
</div>

<h1 id="ordering-by-physical-data-location">Ordering by Physical Data Location</h1>

<p>We can do better. Some file systems, like Linux EXT4, offer an API for fetching information about file extents: <code class="language-plaintext highlighter-rouge">FIEMAP ioctl</code>.
We can use this API to get a data structure that contains information on the physical placement of the file data. 
Then, the physical placement of the beginning of the data can be used to sort the files so that we can process
all files in a single sweep. A great news is that this API is also available for non-root users.</p>

<p>Using <code class="language-plaintext highlighter-rouge">FIEMAP</code> in Rust is easy, because there is already a Rust crate for that: <a href="https://crates.io/crates/fiemap"><code class="language-plaintext highlighter-rouge">fiemap</code></a>. 
The relevant fragment of <code class="language-plaintext highlighter-rouge">fclones</code> code looks like this:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">#[cfg(target_os</span> <span class="nd">=</span> <span class="s">"linux"</span><span class="nd">)]</span>
<span class="k">pub</span> <span class="k">fn</span> <span class="nf">get_physical_file_location</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Path</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">io</span><span class="p">::</span><span class="n">Result</span><span class="o">&lt;</span><span class="nb">Option</span><span class="o">&lt;</span><span class="nb">u64</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="k">mut</span> <span class="n">extents</span> <span class="o">=</span> <span class="nn">fiemap</span><span class="p">::</span><span class="nf">fiemap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">path</span><span class="nf">.to_path_buf</span><span class="p">())</span><span class="o">?</span><span class="p">;</span>
    <span class="k">match</span> <span class="n">extents</span><span class="nf">.next</span><span class="p">()</span> <span class="p">{</span>
        <span class="nf">Some</span><span class="p">(</span><span class="n">fe</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nf">Ok</span><span class="p">(</span><span class="nf">Some</span><span class="p">(</span><span class="n">fe</span><span class="o">?</span><span class="py">.fe_physical</span><span class="p">)),</span>
        <span class="nb">None</span> <span class="k">=&gt;</span> <span class="nf">Ok</span><span class="p">(</span><span class="nb">None</span><span class="p">),</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>I was initially worried that an additional system call for every file would add some initial cost, canceling the gains
from the access ordering. Fortunately it turned out the cost was really low – 50k files could be queried for extents in less than
a second! I guess that the fact the metadata for the files were already queried in an earlier stage, so all the
required information was already in the cache. Fig. 2 shows that despite the higher number of system calls, 
the total time of the task decreased even more, down to about 19 seconds! This is over 10x faster than the earlier release.</p>

<div class="figure">
    <div style="height:9em">
        <canvas id="fiemapOrdering"></canvas>
    </div>
    <script>
    makeBarChartDeferred("fiemapOrdering", "time [s]", "ordering",
        ["by size", "by inode", "by physical location"],
        {"time": [217, 28.43, 19.45]});
    </script>
    <span class="caption"> Fig.2: Impact of physical data ordering on time to find duplicates among 50k+ files stored on a 7200 RPM drive</span>
</div>

<p>The number of transactions per second and throughput reported by <code class="language-plaintext highlighter-rouge">iostat</code> also went up considerably.
Many files are read now in a single disk plate turn.</p>

<pre>
Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda            2424,40     11605,60         0,00      58028          0
sdb               1,00         4,80        11,20         24         56

Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda            2388,20     10436,80         0,00      52184          0
sdb               6,60       356,80        38,40       1784        192

Device             tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda            2397,00     11188,00         0,00      55940          0
sdb               3,20        80,80        56,80        404        284
</pre>

<h1 id="impact-of-parallelism">Impact of Parallelism</h1>

<p>Before introducing the reordering, I’ve found that issuing requests in parallel for reading small (4-64 kB) chunks of data improved speed.
The operating system definitely made a good use of knowing some files in advance and reordered accesses by itself. 
Is it still the case after we order the reads? Maybe giving the operating system a bit more requests in advance could still save some time? 
I thought the system could technically work on fetching the next file while the app is still processing the earlier one.</p>

<p>Unfortunately, at least on my system, this seems to not work as I thought. Fetching files in parallel degraded performance a bit (Fig. 3). The effect
wasn’t as huge as for sequential access of big files, but big enough that I changed the defaults in <code class="language-plaintext highlighter-rouge">fclones 0.9.1</code> to now use always a 
single-thread per HDD device.</p>

<div class="figure">
    <div style="height:9em">
        <canvas id="parallelAccess"></canvas>
    </div>
    <script>
    makeBarChartDeferred("parallelAccess", "time [s]", "# threads",
        [1, 2, 8],
        {"time": [19.45, 25.22, 29.11]});
    </script>
    <span class="caption"> Fig.3: Impact of parallelism on performance of ordered disk access</span>
</div>

<h1 id="summary">Summary</h1>

<p>The order of file I/O requests has a tremendous impact on I/O performance on spinning drives.
If your application needs to process a batch of small files, make sure you request them in the 
same order as their physical placement on disk. If you can’t do it because your file system
or your operating system does not provide physical block placement information, at least
sort the files by their identifiers. If you’re lucky, the identifiers would be highly correlated
with the physical placement of data, and such ordering would still do some magic.</p>

<p>Please let me know in the comments if you tried this and how big improvements you’ve got.</p>

<h1 id="references">References</h1>
<ol class="bibliography"><li><span id="Lunde2009">[1] C. Lunde, H. Espeland, H. Stensland, and P. Halvorsen, “Improving File Tree Traversal Performance by Scheduling I/O Operations in User space,” Dec. 2009, pp. 145–152, doi: 10.1109/PCCC.2009.5403829.</span></li></ol>


  

<style>
#share-buttons { margin-top: 2em; }
#share-buttons > a {   
    display: inline-block;
    vertical-align: baseline;
}
#share-buttons > a > svg { 
    height: 1.2em; 
    width: 1.2em; 
    margin-left: .25em; 
    margin-right: .25em; 
    fill: gray; 
    position: relative; 
    top: .17em; 
}
#share-buttons > span { 
    margin-right: .4em 
}
#share-buttons > a:hover {cursor: pointer;}
#share-buttons > a.facebook:hover > svg {fill: #3B5998;}
#share-buttons > a.twitter:hover > svg {fill: #55ACEE;}
#share-buttons > a.linkedin:hover > svg {fill: #0077b5;}
#share-buttons > a.pinterest:hover > svg {fill: #CB2027;}
#share-buttons > a.mail:hover > svg {fill: #0077b5; }
</style>

<div id="share-buttons">
    <span style="color: gray;">Share on:</span>
    <a class="facebook" title="Share this on Facebook" href="http://www.facebook.com/share.php?u=/disk-access-ordering/" target="_blank"> 
        <svg><use xlink:href="/assets/fontawesome/icons.svg#facebook"></use></svg>
    </a>
    <a class="twitter" title="Share this on Twitter" href="https://twitter.com/intent/tweet?text=/disk-access-ordering/" target="_blank">
        <svg><use xlink:href="/assets/fontawesome/icons.svg#twitter"></use></svg>
    </a>
    <a class="linkedin" title="Share this on Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=/disk-access-ordering/" target="_blank">
        <svg><use xlink:href="/assets/fontawesome/icons.svg#linkedin"></use></svg>
    </a>
    <a class="mail" title="Share this through Email" href="mailto:?&body=/disk-access-ordering/">
        <svg><use xlink:href="/assets/fontawesome/icons.svg#envelope"></use></svg>
    </a>
</div>

  
    <hr>
    
        
      <div id="disqus_thread"></div>
      <script src="/assets/disqus/disqusloader.js"></script>
      <script>        
        disqusLoader('#disqus_thread', { scriptUrl: "//pkolaczk.disqus.com/embed.js" });
      </script> 
    
    <noscript>Please enable JavaScript to view comments.</noscript>
  
</article>


  <footer class="related">
    <div class="previous"><span>Previous Post</span><a href="/estimating-benchmark-errors/">Estimating Benchmark Results Uncertainty</a></div>
    <div class="next"></div>
  </footer>


</body>
</html>
